# PolÃ­ticas de Scheduling

3 estratÃ©gias implementadas para comparaÃ§Ã£o.

## ğŸ“‹ VisÃ£o Geral

| PolÃ­tica | Arquivo | Mecanismo | RÃ©plicas | Resultado Esperado |
|----------|---------|-----------|----------|-------------------|
| **Default** | `1-default/` | Scheduler padrÃ£o | 20 | DistribuiÃ§Ã£o natural |
| **Spreading** | `2-spreading/` | TopologySpreadConstraints | 20 | ~5 pods/node |
| **Anti-Affinity** | `3-anti-affinity/` | Pod Anti-Affinity (hard) | 4 | 1 pod/node |

---

## 1ï¸âƒ£ Default (Baseline)

**Arquivo:** `1-default/deployment.yaml`

**Comportamento:**
- Sem polÃ­ticas de scheduling
- Scheduler padrÃ£o do K8s decide
- Balanceamento bÃ¡sico considerando recursos

**Objetivo:**
- Baseline para comparaÃ§Ã£o
- Ver comportamento "natural" do scheduler

**Resultado esperado:**
- DistribuiÃ§Ã£o razoÃ¡vel mas nÃ£o uniforme
- Tende a equilibrar recursos

---

## 2ï¸âƒ£ Spreading (DistribuiÃ§Ã£o Uniforme)

**Arquivo:** `2-spreading/deployment.yaml`

**Mecanismo:**
```yaml
topologySpreadConstraints:
- maxSkew: 1
  topologyKey: kubernetes.io/hostname
  whenUnsatisfiable: ScheduleAnyway
```

**Comportamento:**
- Garante diferenÃ§a mÃ¡xima de 1 pod entre nodes
- `ScheduleAnyway` = flexÃ­vel, nÃ£o trava

**Objetivo:**
- Balanceamento de carga
- Evitar hotspots
- Usar todos os nodes uniformemente

**Resultado esperado:**
- 20 pods / 4 workers = 5 pods por node
- DistribuiÃ§Ã£o visual clara

**Caso de uso real:**
- APIs stateless
- Workers de background
- AplicaÃ§Ãµes que precisam distribuir carga

---

## 3ï¸âƒ£ Anti-Affinity (Alta Disponibilidade)

**Arquivo:** `3-anti-affinity/deployment.yaml`

**Mecanismo:**
```yaml
podAntiAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchLabels:
        policy: anti-affinity
    topologyKey: kubernetes.io/hostname
```

**Comportamento:**
- **HARD constraint:** NUNCA dois pods no mesmo node
- `required` = obrigatÃ³rio, nÃ£o negociÃ¡vel

**Objetivo:**
- Alta disponibilidade
- TolerÃ¢ncia a falha de node
- Isolamento mÃ¡ximo

**Resultado esperado:**
- MÃ¡ximo 4 pods (1 por worker)
- **LimitaÃ§Ã£o documentada:** Mais rÃ©plicas ficam Pending

**Caso de uso real:**
- Bancos de dados (replicas)
- Redis/Memcached clusters
- AplicaÃ§Ãµes crÃ­ticas

**âš ï¸ Importante:**
- Com 4 workers, mÃ¡ximo 4 pods
- Se `replicas > 4`, os extras ficam Pending
- Isso Ã© comportamento esperado e demonstra limitaÃ§Ã£o real

---

## ğŸš€ Como Usar

```bash
# Deploy uma polÃ­tica
cd k8s-scheduling-lab
./scripts/deploy-policy.sh spreading

# Ver distribuiÃ§Ã£o
make status

# Limpar
make clean

# Testar outra
./scripts/deploy-policy.sh anti-affinity
```

---

## ğŸ“Š ComparaÃ§Ã£o RÃ¡pida

### DistribuiÃ§Ã£o de Pods
- **Default:** Natural (~4-6 por node)
- **Spreading:** Uniforme (5 por node)
- **Anti-Affinity:** Isolado (1 por node)

### UtilizaÃ§Ã£o de Nodes
- **Default:** 4/4 nodes
- **Spreading:** 4/4 nodes
- **Anti-Affinity:** 4/4 nodes (forÃ§ado)

### Trade-offs

| PolÃ­tica | âœ… Vantagem | âŒ Desvantagem |
|----------|-----------|--------------|
| **Spreading** | Balanceamento, resiliÃªncia | Usa todos os nodes (custo) |
| **Anti-Affinity** | HA mÃ¡xima | Limita escalabilidade |

---

## ğŸ” PrÃ³ximos Passos

- [ ] Adicionar mÃ©tricas de latÃªncia inter-pod
- [ ] Testar com diferentes nÃºmeros de rÃ©plicas
- [ ] Simular falha de node
- [ ] Medir tempo de scheduling
- [ ] Comparar com scheduler customizado

---

## ğŸ“š ReferÃªncias

- [Topology Spread Constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)
- [Pod Affinity/Anti-Affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)
- [Scheduling Policies](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/)
