# Pol√≠ticas de Scheduling

4 estrat√©gias implementadas para compara√ß√£o.

## üìã Vis√£o Geral

| Pol√≠tica | Arquivo | Mecanismo | R√©plicas | Resultado Esperado |
|----------|---------|-----------|----------|-------------------|
| **Default** | `1-default/` | Scheduler padr√£o | 20 | Distribui√ß√£o natural |
| **Spreading** | `2-spreading/` | TopologySpreadConstraints | 20 | ~5 pods/node |
| **Anti-Affinity** | `3-anti-affinity/` | Pod Anti-Affinity (hard) | 4 | 1 pod/node |
| **Pod Affinity** | `4-pod-affinity/` | Pod Affinity (preferred) | 20 | Concentra√ß√£o |

---

## 1Ô∏è‚É£ Default (Baseline)

**Arquivo:** `1-default/deployment.yaml`

**Comportamento:**
- Sem pol√≠ticas de scheduling
- Scheduler padr√£o do K8s decide
- Balanceamento b√°sico considerando recursos

**Objetivo:**
- Baseline para compara√ß√£o
- Ver comportamento "natural" do scheduler

**Resultado esperado:**
- Distribui√ß√£o razo√°vel mas n√£o uniforme
- Tende a equilibrar recursos

---

## 2Ô∏è‚É£ Spreading (Distribui√ß√£o Uniforme)

**Arquivo:** `2-spreading/deployment.yaml`

**Mecanismo:**
```yaml
topologySpreadConstraints:
- maxSkew: 1
  topologyKey: kubernetes.io/hostname
  whenUnsatisfiable: ScheduleAnyway
```

**Comportamento:**
- Garante diferen√ßa m√°xima de 1 pod entre nodes
- `ScheduleAnyway` = flex√≠vel, n√£o trava

**Objetivo:**
- Balanceamento de carga
- Evitar hotspots
- Usar todos os nodes uniformemente

**Resultado esperado:**
- 20 pods / 4 workers = 5 pods por node
- Distribui√ß√£o visual clara

**Caso de uso real:**
- APIs stateless
- Workers de background
- Aplica√ß√µes que precisam distribuir carga

---

## 3Ô∏è‚É£ Anti-Affinity (Alta Disponibilidade)

**Arquivo:** `3-anti-affinity/deployment.yaml`

**Mecanismo:**
```yaml
podAntiAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchLabels:
        policy: anti-affinity
    topologyKey: kubernetes.io/hostname
```

**Comportamento:**
- **HARD constraint:** NUNCA dois pods no mesmo node
- `required` = obrigat√≥rio, n√£o negoci√°vel

**Objetivo:**
- Alta disponibilidade
- Toler√¢ncia a falha de node
- Isolamento m√°ximo

**Resultado esperado:**
- M√°ximo 4 pods (1 por worker)
- **Limita√ß√£o documentada:** Mais r√©plicas ficam Pending

**Caso de uso real:**
- Bancos de dados (replicas)
- Redis/Memcached clusters
- Aplica√ß√µes cr√≠ticas

**‚ö†Ô∏è Importante:**
- Com 4 workers, m√°ximo 4 pods
- Se `replicas > 4`, os extras ficam Pending
- Isso √© comportamento esperado e demonstra limita√ß√£o real

---

## 4Ô∏è‚É£ Pod Affinity (Concentra√ß√£o)

**Arquivo:** `4-pod-affinity/deployment.yaml`

**Mecanismo:**
```yaml
podAffinity:
  preferredDuringSchedulingIgnoredDuringExecution:
  - weight: 100
    podAffinityTerm:
      labelSelector:
        matchLabels:
          policy: pod-affinity
      topologyKey: kubernetes.io/hostname
```

**Comportamento:**
- **SOFT preference:** Prefere nodes que j√° t√™m pods do app
- `preferred` = tenta, mas n√£o obriga
- `weight: 100` = alta prioridade

**Objetivo:**
- Concentrar pods em poucos nodes
- Otimiza√ß√£o de custo (permite desligar nodes vazios)
- Simula comportamento de binpacking

**Resultado esperado:**
- Pods concentrados em 1-2 nodes
- Outros nodes ficam ociosos ou com poucos pods

**Caso de uso real:**
- Otimiza√ß√£o de custo cloud
- Consolida√ß√£o de workloads
- Prepara√ß√£o para scale-down

**‚ö†Ô∏è Nota t√©cnica:**
- N√£o √© binpacking real (MostAllocated plugin)
- √â simula√ß√£o via Pod Affinity
- Comportamento similar, mas n√£o id√™ntico

---

## üöÄ Como Usar

```bash
# Deploy uma pol√≠tica
cd k8s-scheduling-lab
./scripts/deploy-policy.sh spreading

# Ver distribui√ß√£o
make status

# Limpar
make clean

# Testar outra
./scripts/deploy-policy.sh anti-affinity
```

---

## üìä Compara√ß√£o R√°pida

### Distribui√ß√£o de Pods
- **Default:** Natural (~4-6 por node)
- **Spreading:** Uniforme (5 por node)
- **Anti-Affinity:** Isolado (1 por node)
- **Pod Affinity:** Concentrado (15+ em 1 node)

### Utiliza√ß√£o de Nodes
- **Default:** 4/4 nodes
- **Spreading:** 4/4 nodes
- **Anti-Affinity:** 4/4 nodes (for√ßado)
- **Pod Affinity:** 1-2/4 nodes

### Trade-offs

| Pol√≠tica | ‚úÖ Vantagem | ‚ùå Desvantagem |
|----------|-----------|--------------|
| **Spreading** | Balanceamento, resili√™ncia | Usa todos os nodes (custo) |
| **Anti-Affinity** | HA m√°xima | Limita escalabilidade |
| **Pod Affinity** | Economia, consolida√ß√£o | Blast radius alto |

---

## üîç Pr√≥ximos Passos

- [ ] Adicionar m√©tricas de lat√™ncia inter-pod
- [ ] Testar com diferentes n√∫meros de r√©plicas
- [ ] Simular falha de node
- [ ] Medir tempo de scheduling
- [ ] Comparar com scheduler customizado

---

## üìö Refer√™ncias

- [Topology Spread Constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)
- [Pod Affinity/Anti-Affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)
- [Scheduling Policies](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/)
