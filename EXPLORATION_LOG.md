# Log de Explora√ß√£o - K8s Scheduling Lab

**Per√≠odo:** 03-07 Dezembro 2024
**Objetivo:** Explorar pol√≠ticas de scheduling para definir tema de TCC

---

## üéØ Motiva√ß√£o

Antes de definir tema espec√≠fico do TCC, criar ambiente de experimenta√ß√£o funcional para:
- Entender diferen√ßas pr√°ticas entre pol√≠ticas
- Validar m√©tricas mensur√°veis
- Identificar perguntas de pesquisa interessantes
- Ter demonstra√ß√£o pronta para orientador

---

## üõ†Ô∏è Setup Inicial

### Ambiente
- **Cluster:** Kind v1.27.3
- **Nodes:** 1 control-plane + 4 workers
- **Monitoring:** kube-prometheus-stack (Prometheus + Grafana)
- **Workload:** Nginx customizado com labels
- **Host:** [preencher: CPU, RAM, OS]

### Instala√ß√£o
```bash
make setup  # ~10min
make grafana  # port-forward
```

**Status:** ‚úÖ Funcionou sem problemas

---

## üß™ Experimentos Realizados

### Experimento 1: Default (Baseline)

**Data:** [preencher]

**Configura√ß√£o:**
- 20 pods
- Sem pol√≠ticas de scheduling
- Scheduler padr√£o decide

**Comando:**
```bash
./scripts/deploy-policy.sh default
```

**Resultado - Distribui√ß√£o:**
```
[colar output do kubectl get pods -o wide]
```

**Pods por node:**
```
[colar sa√≠da do make status]
```

**Observa√ß√µes:**
- [anotar o que chamou aten√ß√£o]
- [distribui√ß√£o uniforme ou desbalanceada?]
- [algum node ficou sem pods?]

**Screenshot Grafana:**
`dashboards/screenshots/01-default.png`

---

### Experimento 2: Spreading (Distribui√ß√£o Uniforme)

**Data:** [preencher]

**Configura√ß√£o:**
- 20 pods
- TopologySpreadConstraints, maxSkew=1
- whenUnsatisfiable: ScheduleAnyway

**Comando:**
```bash
make clean
./scripts/deploy-policy.sh spreading
```

**Resultado - Distribui√ß√£o:**
```
[colar output]
```

**Observa√ß√µes:**
- Distribui√ß√£o foi realmente uniforme? (~5 pods/node)
- Diferen√ßa vis√≠vel vs default?
- Tempo de scheduling not√°vel?

**Screenshot Grafana:**
`dashboards/screenshots/02-spreading.png`

---

### Experimento 3: Anti-Affinity (Alta Disponibilidade)

**Data:** [preencher]

**Configura√ß√£o:**
- 4 pods (limitado pelo n√∫mero de nodes)
- Pod Anti-Affinity hard (required)
- Garante m√°ximo 1 pod por node

**Comando:**
```bash
make clean
./scripts/deploy-policy.sh anti-affinity
```

**Resultado - Distribui√ß√£o:**
```
[colar output]
```

**Observa√ß√µes:**
- Exatamente 1 pod por node? ‚úÖ
- Se aumentar r√©plicas pra 5, o 5¬∫ fica Pending?
- Limita√ß√£o clara demonstrada?

**Screenshot Grafana:**
`dashboards/screenshots/03-anti-affinity.png`

**Teste adicional (opcional):**
```bash
# Editar deployment pra 5 r√©plicas
kubectl scale deployment nginx-test --replicas=5
kubectl get pods  # Ver 1 Pending
```

---

### Experimento 4: Pod Affinity (Concentra√ß√£o)

**Data:** [preencher]

**Configura√ß√£o:**
- 20 pods
- Pod Affinity preferred (soft), weight=100
- Prefere nodes que j√° t√™m pods do app

**Comando:**
```bash
make clean
./scripts/deploy-policy.sh pod-affinity
```

**Resultado - Distribui√ß√£o:**
```
[colar output]
```

**Observa√ß√µes:**
- Concentrou em 1-2 nodes?
- Ou distribuiu mais do que esperado?
- Diferen√ßa vs default √© clara visualmente?

**Screenshot Grafana:**
`dashboards/screenshots/04-pod-affinity.png`

---

## üìä Compara√ß√£o Visual

| Pol√≠tica | Pods/Node (aprox) | Nodes Usados | Uniformidade |
|----------|-------------------|--------------|--------------|
| Default | [preencher] | [ex: 4/4] | [m√©dia/baixa/alta] |
| Spreading | ~5 por node | 4/4 | Alta |
| Anti-Affinity | 1 por node | 4/4 | Perfeita |
| Pod Affinity | [preencher] | [ex: 2/4] | Baixa |

---

## üí° Descobertas

### T√©cnicas

1. **Anti-Affinity √© limitado pelo n√∫mero de nodes**
   - Constraint hard n√£o negocia
   - Com 4 workers, m√°ximo 4 pods
   - Pods extras ficam Pending (comportamento esperado)

2. **Pod Affinity n√£o √© binpacking real**
   - Simula comportamento via preferredDuringScheduling
   - Mas n√£o √© o plugin MostAllocated do scheduler
   - Concentra, mas n√£o 100%

3. **Spreading funciona muito bem visualmente**
   - Diferen√ßa clara vs default
   - maxSkew=1 garante uniformidade
   - M√©tricas visuais s√£o suficientes

4. **[Adicionar suas descobertas]**

### M√©tricas Vi√°veis

‚úÖ **O que d√° pra medir facilmente:**
- Pods por node (visual, claro)
- Nodes utilizados (efici√™ncia)
- Pods Pending (limita√ß√µes)
- Timeline de distribui√ß√£o (Grafana)

‚ö†Ô∏è **O que seria mais trabalho:**
- StdDev matem√°tico preciso
- Lat√™ncia inter-pod
- Tempo de scheduling
- M√©tricas de resili√™ncia (precisa simular falhas)

### Limita√ß√µes do Setup

1. **Kind n√£o √© produ√ß√£o:**
   - Nodes s√£o containers, n√£o VMs
   - Recursos compartilhados com host
   - M√©tricas de CPU/RAM s√£o aproximadas

2. **Escala pequena:**
   - 4 workers √© suficiente pra demonstrar conceito
   - Mas n√£o testa comportamento em escala (100+ nodes)

3. **Workload sint√©tico:**
   - Nginx b√°sico n√£o simula padr√µes reais
   - Sem I/O, sem comunica√ß√£o inter-pod
   - Mas serve pro prop√≥sito de visualizar distribui√ß√£o

---

## ü§î Perguntas para o Orientador (Giovanni)

### Sobre Escopo
1. Focar em **compara√ß√£o visual** ou **an√°lise quantitativa**?
2. 4 nodes √© suficiente ou precisa escalar (ex: 10 nodes)?
3. Adicionar cen√°rios de falha (simular node down)?

### Sobre Tema
1. **Op√ß√£o A:** "Compara√ß√£o emp√≠rica de pol√≠ticas de scheduling"
   - Foco: M√©tricas, gr√°ficos, an√°lise
   - Rigor: Mais cient√≠fico, precisa estat√≠stica

2. **Op√ß√£o B:** "Playbook: Como escolher pol√≠ticas baseado em requisitos"
   - Foco: Guia pr√°tico, trade-offs
   - Rigor: Mais engenharia, menos formal

3. **Op√ß√£o C:** "Ferramenta de recomenda√ß√£o de pol√≠tica"
   - Foco: Implementa√ß√£o, automa√ß√£o
   - Rigor: Mais c√≥digo, menos an√°lise

Qual dire√ß√£o faz mais sentido pro TCC?

### Sobre Metodologia
1. M√©tricas atuais s√£o suficientes?
2. Precisa de an√°lise estat√≠stica formal (m√©dia, IC 95%)?
3. Quantas execu√ß√µes por cen√°rio? (atualmente 1, poderia ser 10+)

---

## üéØ Poss√≠veis Temas de TCC

### Tema 1: An√°lise Comparativa Quantitativa

**T√≠tulo:** "An√°lise Comparativa de Pol√≠ticas de Scheduling em Kubernetes: Estudo Emp√≠rico"

**Pergunta de pesquisa:**
- Como diferentes pol√≠ticas afetam distribui√ß√£o de pods?
- Quais trade-offs entre utiliza√ß√£o de recursos e resili√™ncia?

**Metodologia:**
- Ambiente controlado (Kind)
- 10+ execu√ß√µes por cen√°rio
- An√°lise estat√≠stica (m√©dia, desvio, IC)
- M√©tricas: distribui√ß√£o, nodes usados, efici√™ncia

**Pros:**
- Cient√≠fico, public√°vel
- M√©tricas claras
- Reproduz√≠vel

**Contras:**
- Precisa rigor estat√≠stico
- Mais trabalho em an√°lise
- Menos aplic√°vel √† ind√∫stria

---

### Tema 2: Guia de Decis√£o (Playbook)

**T√≠tulo:** "Playbook de Scheduling Kubernetes: Guia de Decis√£o Baseado em Requisitos"

**Pergunta de pesquisa:**
- Quando usar cada pol√≠tica?
- Quais os trade-offs pr√°ticos?
- Como escolher baseado em requisitos de neg√≥cio?

**Metodologia:**
- Cen√°rios pr√°ticos (custo, HA, performance)
- Demonstra√ß√µes visuais
- An√°lise qualitativa de trade-offs
- Guia de decis√£o estruturado

**Pros:**
- √ötil pra ind√∫stria
- Demonstr√°vel
- Menos rigor formal necess√°rio

**Contras:**
- Menos cient√≠fico
- Mais subjetivo
- Contribui√ß√£o acad√™mica menor

---

### Tema 3: Sistema de Recomenda√ß√£o

**T√≠tulo:** "Sistema de Recomenda√ß√£o de Pol√≠ticas de Scheduling Baseado em Caracter√≠sticas do Workload"

**Pergunta de pesquisa:**
- √â poss√≠vel automatizar escolha de pol√≠tica?
- Quais caracter√≠sticas do workload importam?
- Como criar ferramenta de apoio √† decis√£o?

**Metodologia:**
- Classifica√ß√£o de workloads
- Modelo de decis√£o (regras ou ML)
- Implementa√ß√£o de CLI/webapp
- Valida√ß√£o com casos reais

**Pros:**
- Contribui√ß√£o t√©cnica clara
- Diferenciado
- Bom pra portfolio

**Contras:**
- Mais complexo
- Precisa validar modelo
- Risco de over-engineering

---

## üìÖ Pr√≥ximos Passos

- [ ] **Hoje (07/12):** Finalizar screenshots, exportar dashboard
- [ ] **08/12:** Email pro Giovanni com demo
- [ ] **Semana 09-13/12:** Reuni√£o com orientador
- [ ] **Pausa dez:** Foco no novo emprego
- [ ] **Jan/2025:** Retomar com tema definido

---

## üìö Leituras Realizadas

- [ ] "A survey of Kubernetes scheduling algorithms" (Journal of Cloud Computing, 2023)
- [ ] "Optimization of Task-Scheduling Strategy in Edge K8s" (MDPI, 2023)
- [ ] Docs oficiais: Topology Spread Constraints
- [ ] Docs oficiais: Pod Affinity/Anti-Affinity
- [ ] [Adicionar outros]

---

## üîó Links √öteis

- [Kubernetes Scheduler](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/)
- [Scheduler Plugins](https://github.com/kubernetes-sigs/scheduler-plugins)
- [KEP-895: TopologySpreadConstraints](https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread)

---

**√öltima atualiza√ß√£o:** [data]
